<!DOCTYPE HTML>
<html>

<head>
    <meta charset="utf-8">

    <meta name="description" content="Docker vs Vagrant" />
    <meta name="keywords" content="SETT, OCI, Docker, Vagrant" />
    <meta name="author" content="Yong Fu" />
    <title>SETT Feb 2017 - CloudML</title>

    <link rel="alternate" type="application/rss+xml" title="RSS" 
href="http://ociweb.com/sett/rss.xml" />
    <link href="styles/SETT.css" rel="stylesheet" type="text/css" />


    <!--Used for syntax highlighting.  -->
    <link href="http://alexgorbatchev.com/pub/sh/current/styles/shCore.css" 
rel="stylesheet" type="text/css" />
    <link href="http://alexgorbatchev.com/pub/sh/current/styles/shThemeDefault.css" 
rel="stylesheet" type="text/css" />
</head>

<body>
    <!--#include virtual="header.shtml" -->

    <h1>Leveraging Prediction API's and CloudML for practical applications</h1>

    <p class="author">
        by
        <br/> Data Analytics Group
        <br/>Object Computing, Inc. (OCI)
    </p>

    <h2>Introduction</h2>
    <p>
      Google Cloud Platform and CloudML
    </p>
    <p>
      Machine learning workflow
    </p>
    <p>
      Machine learning workflow on Google Cloud Platform
    </p>
    
    <h2>Goole Prediction APIS</h2>
	
	<p>
	Google prediction API offers machine learning as a service. It learns from a user given 
	training data and provides pattern-matching and machine learning capabilities. 
	The prediction API can predict a numeric value or a categorical value that describes a 
	new data example. Using these capabilities there is a possibility of building applications 
	from spam detection to recommendation engines without actually worrying about building a model.
	</p>
	
	<p>
	The following are the range of use-cases that can be built using the capabilitis of 
	google prediction API:
	</p>
	<ul>
		<li> Predict future trends from a given historical series of data.
		<li> Detect if a given email is a spam
		<li> Recommend a product/movie to the user based on the interests of a similar user.
		<li> Identify whether a given user will default based on the credit usage history of the user.
		<li> Activity detection from smartphone sensor data using labelled activity data-sets. 
	</ul>
	
	<p>
	On a given labelled training dataset. Prediction API performs the following specific tasks:
	</p>
	<ul>
		<li> Given a new item, predict a numeric value for that item, based on similar valued examples in its training data (regression).
		<li> Given a new item, choose a category that describes it best, given a set of similar categorized items in its training data (classification).
	</ul>
	
	<p>
	The bottomline of all the above discussed applications is to predict a world state parameter (target label value)
	for an unknown example based on the past labelled data examples. The prediction API will take care
	of building best suitable models using google's fast and reliable computing resources. Most prediction
	queries take less then 200ms.
	</p>
	
	<h3>How does the prediction API works?</h3>
	<p>
	The implementation of the Google's prediction API can be pretty much defined as a blackbox approach. 
	There is no control on the model-selection, model-tuning and things that happen in the background during training. 
	The model configuration is restricted to specifying the class of problem whether it 
	is "Classification" Vs "Regression" during the data preparation for training process. 
	On a very highlevel, the information flow in the prediction API looks as shown in the following:
	</p>
	
	<div id="container">
      <a href="#">
	<figure>
	  <img src="./figures/prediction_api.jpg" alt="inception" width="1000" height="400"/>
	  <figcaption>Information flow of Predcition API</figcaption>
	</figure>
      </a>
    </div>
	
	<p>
	Input features can contain any type of data, and the API does not impose any constraints on input data types or 
	any configuration process. The important step is data preparation, as per the specified format that prediction 
	API will accept for training. The data simply looks like a big table, where each record is an input data example 
	and the labels (target value) are specified in the very first column in the training set. The only difference between 
	training and the testing sets is the first column will not be present in the testing set. 
	The biggest disadvantage however with this type of implementation is you cannot predict two different world state parameters (labels) at the same time, which otherwise 
	can be done with your own model implementaion. 
	</p>
	
	<p>
	Prediction API offers a simple way to train machine learning models through a RESTful interface. The training phase 
	is initialized by calling "trainedmodels.insert" method. 
	The training phase is asynchronous, where you can poll the API using "prediction.trainedmodels.get" method 
	to check the status of the training. In the response, when "trainingStatus" property changed to "Done" from "Running",
	when the model training is completed. Only after this, you can start making predictions for new data examples.
	</p>
	
	<p>
	The API provides with "trainedmodels.analyze method" which specifies the "modelDescription" that contains the 
	confusionMatrix in the JSON format. It will not provide any additional statistics like precision, recall and F-score which
	you can manually calculate. 
	</p>
	
	<p>
	For the model built, you can make predictions for new example data-sets by calling "trainedmodels.predict" method, this returns
	the parameters "outputLabel" (numeric or String) and "outputMulti" which is a probability measure for each prediction class. The "outputMulti" 
	is so useful in making final prediction programmatically by inspecting each class's probability taking the context of the problem in to consideration. 
	</p>
	
	<h3>Who is the audience for Prediction API's?</h3>
	
	<p>
	Traditionally to work with a machine leaning problem, you normally start with preprocessing your data
	by performing some steps like dealing with missing data, input normalization, dataset splitting in to training and validation sets. 
	Then comes the step of model selection, based on the correlations in your training data, you identify a model that fits your scenario. 
	</p>
	
	<p>
	With the prediction API, you donâ€™t need to worry about these steps, since the API automatically trains and tests a lot of complex models, tuned 
	with different parameters, so that the best one will be chosen for the final evaluation. The model which API finally comes up with would have been 
	the one you end up after so many iterations of tuning which involves manual selection of parameters. Even, the model evaluation is handled by the API itself. 
	All you need to worry and work about is to provide a valid data source in the required format, to the API. 
	Google's blackbox implementaion approach for prediction API makes it clear that it wants to ease the implementation to non-coders also. In a way, the mathematical
	expertise required to build, analyze the machine learning models, is eliminated by the capabilities of the API. One 
	can invest more time in problem formulation, data-collection and making a flaw-less end to end implementation, with prediction API.
	</p>

	<h3>Prediction API: A real use-case for sentiment analysis</h3>
	
	<p>
	To illustrate the prediction API, a simple binary classification problem is shown to classify positive or negative 
	sentiment from the  Twitter sentiment analysis corpus dataset (http://thinknook.com/twitter-sentiment-analysis-training
	-corpus-dataset-2012-09-22/). 
	</p>
	
	<p>
	For working with a machine learning problem, a most important and time-consuming part is defining the problem appropriately 
	and preparing the dataset accordingly. Before all that, it should be analyzed and made sure that if the problem can be solved 
	using machine learning approach in the first place. After all, not every problem is solvable in using machine learning. 
	</p>
	
	<p>
	There are two main things you need to clarify and make sure before starting the problem: 
	</p>
	<ul>
		<li>what is the type of the problem regression/classiffication, and accordingly making sure what are you going to predict/classify
		<li>making necessary assumptions, while not affecting the scope of the problem
	</ul>
	
	<p>
	For our current problem, since we are going to identify a predefined label "Negative" or "Positive", this is clearly a classification problem. 
	The second point is important as you can have a redundant data which will inturn affects the model accuracy. Typically, 
	if the features are too specific to the current data-set, you may end up with a very large generalization error, resulting in overfitting. 
	This is the piece you need to handle yourself, to better assist the Prediction API predict accurate labels for your test inputs. 
	However, preprocessing step is not mandatory for using Prediction API, you need it to build a good model corresponding to your training set.
	</p>
	
	<p>
	The data-set chosen for the current problem contains about 1.5 million rows,
	and 4 columns. Usually, there may be columns not of interest to a specific problem, so filtering those by including only 
	relevant features will lead to a better model. Accordingly we prepare our data and make sure to include the label in the first column as mentioned in the API 
	implementation above. We now setup the project, which involves creating a cloud platform project (you can also build on top of an existing one), 
	enable billing and enable prediction API for the project.
	</p>
	Note: A  globally unique project id is choses for project name and a number is assigned when the cloud platform  project is created. Detailed description
	on this are provided at https://developers.google.com/ad-exchange/rtb/open-bidder/google-app-guide.

	<p>
	Then create a bucket with globally unique name and add the training set file as 'csv' file to the bucket. For training the model, "prediction.trainedmodels.insert" method is called, 
	passing a unique name for this predictive model, and the bucket location of the training data as shown below. A full list of the methods are provided at https://developers.google.com/apis-explorer/#p/prediction/v1.6/prediction.
	</p>
<div>
    <pre class="prettyprint">
	POST https://www.googleapis.com/prediction/v1.6/projects/oci-analytics/trainedmodels?key={YOUR_API_KEY}
	{
		"id": "sentiment-identifier_12500",
		"storageDataLocation": "oci-prediction_api-demo/twitter_data_12500.csv"
	}
    </pre>
    <figcaption>Request format for initializing the training
</div>

<p>

A successful response looks like:
<div>
    <pre class="prettyprint">
	Response 200
	- Show headers -
	{
		"kind": "prediction#training",
		"id": "sentiment-identifier_12500",
		"selfLink": "https://www.googleapis.com/prediction/v1.6/projects/oci-analytics/trainedmodels/sentiment
		-identifier_12500",
		"storageDataLocation": "oci-prediction_api-demo/twitter_data_12500.csv"
	}
    </pre>
    <figcaption>Response for the training request
</div>

<p>
To check the status of Training, use the "prediction.trainedmodels.get" method, by passing the ID of the predictive model as shown below.
<!-- GET https://www.googleapis.com/prediction/v1.6/projects/[PROJECT_ID]/trainedmodels/sentiment-identifier -->
</p>

<div>
    <pre class="prettyprint">
		GET https://www.googleapis.com/prediction/v1.6/projects/[PROJECT_ID]/trainedmodels/sentiment-identifier
    </pre>
    <figcaption>Querying about the status of training
</div>

<p>
After the training is complete, you can send queries to the service to be evaluated against the predictive model. To do so, call the "prediction.trainedmodels.predict" method, 
passing the name of the model and the query.
</p>


<div>
    <pre class="prettyprint">
		POST https://www.googleapis.com/prediction/v1.6/projects/oci-analytics/trainedmodels/sentiment-identifier_12500/predict?key={YOUR_API_KEY}
		{
			"input": {
				"csvInstance": [
					"I am worried about today's game..."
				]
			}
		}
    </pre>
    <figcaption>Sending a prediction query to the Prediction API
</div>


<div>
    <pre class="prettyprint">
		200
		- Show headers -
		{
			"kind": "prediction#output",
			"id": "sentiment-identifier_12500",
			"selfLink": "https://www.googleapis.com/prediction/v1.6/projects/oci-analytics/trainedmodels/sentiment-identifier_12500/predict",
			"outputLabel": "NEGATIVE",
			"outputMulti": [
			{
				"label": "NEGATIVE",
				"score": "0.696235"
			},
			{
				"label": "POSITIVE",
				"score": "0.303765"
			}
			]
		}
    </pre>
    <figcaption>Prediction response containing the prediction label and probability scores
</div>
	
	<h3>Summary of the Prediction API</h3>
	<p>
	As of now, the prediction API seems to be abstracted very much to the developers. The control is only
	on the data preparation and adding additional data-sets for updating the model, these forms the either 
	end-points of the machine learning pipeline. Optimistically, a key advantage with this approach is, it saves lot of
	time in building the models, and also the flexibility of adding additional data-sets even after the training is completed, 
	resulting in model update on the fly.
	</p>
	
    <h2>Machine learning workflow on Google Cloud Platform</h2>
	
    If users prefer or must train their own model and make prediction
    base the model, components of Google Cloud Platform (GCP) could be
    coordinated to support this requirement.  In this section we
    describe how to prepare data, train and serve custimzed model
    directly on GCP through cooperation between different
    components. The machine learning task example used here is to
    identify flowers based on a small data set flower images. The code
    comes from
    the <a 
href="https://github.com/GoogleCloudPlatform/cloudml-samples/tree/master/flowers">samples
    of CloudML</a>.
    <div id="container">
      <a href="#">
	<figure>
	  <img src="./figures/cloudml-pl.jpg" alt="cloudml-pl" width="400" height="400"/>
	  <figcaption>Build and train model by GCP components 
	</figure>
      </a>
    </div>
    The above figure depicts the GCP components involved in machine
    learning workflow. To initiate and control the workflow, users can
    interact with other GCP components through commandline or
    Datalab. GCP provides a comprenhensive commands to manage stages
    of machine learning workflow. For example,
    <code>gcloud beta ml train </code> will submit a train task into
    GCP (Details of parameters in later section). Datalab is a highly
    customized Python notebook engine (based
    on <a href="https://ipython.org/">IPython</a>) which adds support
    libraries and other enivronment to use GCP directly. It is
    convenient to data scientists or researchers who are more familiar
    to Python script environment to run the workflow. Dataflow is a
    core component to process big data on GCP. It provides a highly
    parallel and fault tolerant environment hence it is suitable for
    data preprocessing before running machine learning tasks. Machine
    learninng is the main component for machine learning tasks and
    used for model training and serving. The training data and
    intemediate and final model can be stored on Storage component.
    
    <h3>Dataflow</h3>

    Dataflow is a collection of SDKs and a managed service for
    building parallelized data processing pipelines. It can support
    both streaming and batch execution of pipeline.

    Major benefits of Dataflow are that 1) it provides a simple
    programming model based on concepts of pipeline, collection and
    . This programming model is open sourced as <a href="">Apache
    Beam</a>. 2) it is an autoamted scalable service so users do not
    consider resources management at the time of programming. In a
    machine learning task dataflow can be used as data preprocessing,
    features extracting.

    <h4>Apache Beam</h4>
    Apache Beam provides a high level abstract of both of batch and
    streaming data processing and implements  
     <a href="http://www.vldb.org/pvldb/vol8/p1792-Akidau.pdf">Dataflow
     model</a>. Beam defines a set of core primitives and rich windows
     and streaming operations. Since most tasks of machine learning
     model training use batch operations to feed the data to the
     training system so in this article we only focus on the
     part related to batch training in Beam.

     The core primitives are defined in Beam is
     <ul>
       <li><b>ParDo</b> for parallel processing. Each input element
	 to be processed (which itself may be a finite collection) is provided
	 to a user-defined function (called a DoFn in Dataflow), which can
	 yield zero or more output elements per input</li>
       <li><b>GroupByKey</b> to generate key-grouping (key, value) pairs.</li>
     </ul>
    In batch mode these two core operation can be easily applied to
    the operation on the data set while in streaming mode
    some complicate widnow semantics must be associated
    with <code>GroupByKey</code>.

    Other key abstracts in beam includes
    <ul>
      <li><b>Pipeline</b> A Pipeline encapsulates your entire
      data processing task, from start to finish. This includes
      reading input data, transforming that data, and writing output
      data. </li>
      <li><b>PCollection</b> A PCollection represents a
      distributed data set that your Beam pipeline operates on.  The
      initial PCollection of the pipeline can be created by reading
      reading data from an external data source and in-memory
      data. Note that PCollection is only necessary in Java API but
      not in Python API. </li>

      <li><b>Transform</b> A Transform represents a data
      processing operation, or a step, in your pipeline. Every
      Transform takes one or more input and perfroms a processing
      function on the elements of input and produce results. </li>

      <li><b>I/O Source and Sink</b> Beam provides Source and
      Sink APIs to represent reading and writing data,
      respectively. They can used as the initial data source and final
      data sink. </li>
    </ul>
       
    <h4>Data preprocessing and feature extracting</h4>
    Before feeding into model training, some preprocessing step are
    often necessary. These steps can be done in Dataflow and
    implemented based on the model of Beam. In flower example the
    preprocessing steps include:
    <ul>
      <li><em>Extract label</em> Read labels from the dictionary and
      assign id to them.</li>
      <li><em>Convert format</em> Conver the image format to JPEG.</li>
      <li><em>Generate training data</em> This step actually includes
      some substeps. First the image are resized and normalize for
      convience of model training. Second the features are extracting
      from the images through a
      pretained <a href="https://github.com/tensorflow/models/tree/master/inception">Google
      inception model</a>. Finally the image (in csv format), id and
      features are assemble into a data structure
      name <code>TFExample</code> and write to Cloud Storage. </li>
    </ul>
    
    <div>
    <pre class="prettyprint">
    input_source = beam.io.TextFileSource(
       opt.input_path, strip_trailing_newlines=True) 
    _ = (p
       | 'Read input' >> beam.Read(input_source)
       | 'Parse input' >> beam.Map(lambda line: csv.reader([line]).next())
       | 'Extract label ids' >> beam.ParDo(ExtractLabelIdsDoFn(), 
beam.pvalue.AsIter(labels))
       | 'Read and convert to JPEG' >> beam.ParDo(ReadImageAndConvertToJpegDoFn())
       | 'Embed and make TFExample' >> beam.ParDo(TFExampleFromImageDoFn())
       | 'Save to disk' >> SaveFeatures(opt.output_path))
      </pre>
    <figcaption>Configure Beam Pipeline for flower example
    </div>
    After submitting this pipeline on Dataflow through its job API, it
    launches a Kubernete containers cluster. On this cluster, Dataflow
    automatically generate parallel tasks according to Beam
    primitives, for example ParDo and assign these tasks to different
    containers. When runing the pipeline Dataflow takes care of
    resources management, fault tolerance and other routine cluster
    operations. Simply put Dataflow is a management service to submit,
    run and maintain your data precessing pipeline.

    Although in the machine learning task Dataflow is mainly
    responsible to data preprocessing and feature engineering, some
    simple model training tasks can be implemented on Beam. However
    since Dataflow focus on parallel data processing it may not be
    efficient to implement a model training algorithm directly on
    Dataflow. For normal model training and serving task, GCP Cloud
    Machine Learning is a suitable service.
      
    <h3>Machine Learning</h3>
    GCP Cloud Machine learning (CloudML) is the core component to
    train and serve models. Behind the hood, CloudML like Beam is a
    managed service running on a kubernetes cluster. And like Beam
    too, the training and serving workflow model is implemented on
    Tensorflow, a distributed framework to support machine learning and deep
    learning tasks. 

    <h4>Tensorflow</h4>
    <p>
    TensorFlow is an interface for expressing
    machine learning algorithms, and an implementation for executing
    such algorithms.  Tensorflow aims to run machine learning
    algorithms on a wide variety of heterogenous systems, from mobile
    devices to large scale distributes systems without little or no
    efforts on portability. Tensorflow is designed to be flexible and
    express a broad spectrum of algorithms, especially training and
    inference algorithms for deep neural network models. It can be
    used in research and production environments across areas such as
    speech recognition, computer vision, robotics, information
    retrieval, natural language processing, geographic information
    extraction, and computational drug discovery.
    </p>
    <p>
    The programming model of TensorFlow is a directed <em>graph</em>, which is
    composed of a set of <em>nodes</em>. The graph represents a dataflow
    computation, with extensions for allowing some kinds of nodes to
    maintain and update persistent state and for branching and looping
    control structures. A more intuitive explanasion of TensorFlow
    model is that graph represents high level structure of machine
    learning algorithm and the node represents basic operations in the
    algorithm.  A state-of-the-art image recognition deep neural
    network, inception, used by the flower model can be visualized as a
    directed graph as following:
    </p>
    <div id="container">
      <a href="#">
	<figure>
	  <img src="./figures/inception_v3_architecture.png" alt="inception" width="1000" 
height="400"/>
	  <figcaption>TensorFlow model of deep learning network in
	  flower example </figcaption>
	</figure>
      </a>
    </div>
    Other key abstracts in TensorFlow are <em>tensor</em>
    and <em>device</em>. A tensor is a typed, multidimensional array,
    flowing throught and storing intermediate computation results
    between nodes. A device is the abstract of the hardware running the
    node. In a distributed system a worker may contain several
    devices. Each of them represents a hardware resource to run the
    computation, for example, device of <em>CPU</em>
    or <em>GPU</em>. A core function of TensorFlow is to assign the
    computation to different devices so to spread the computation of
    the whole graph to the whole distributed system.

    To interact with users, TensorFlow provides the abstract
    of <em>Session</em>. To create a computation graph, the Session
    interface supports to augment the current graph by adding
    additional nodes (the initial graph when a session is created is
    empty). Tensor can not be used to persistate the results so there
    is anothe abstract <em>Session</em> to allow user store or input
    the data into the graph. The following code snippet is small
    example of a TensorFlow model and its visualization. 
    
    <pre class="prettyprint">
      import tensorflow as tf
      b = tf.Variable(tf.zeros([100]))                   # 100-d vector, init to zeroes
      W = tf.Variable(tf.random_uniform([784,100],-1,1)) # 784x100 matrix w/rnd vals
      x = tf.placeholder(name="x")                       # Placeholder for input
      relu = tf.nn.relu(tf.matmul(W, x) + b)             # Relu(Wx+b)
      C = [...]                                          # Cost computed as a function of 
Relu
      s = tf.Session()
      for step in xrange(0, 10):
         input = ...construct 100-D input array ...      # Create 100-d vector for input
         result = s.run(C, feed_dict={x: input})         # Fetch cost, feeding x=input
         print step, result
      </pre>
    <figcaption>A simple example of TensorFlow model</figcaption>
 
    <div id="container">
      <a href="#">
	<figure>
	  <img src="./figures/simple-tf.jpg" alt="simple" width="220" height="400"/>
	  <figcaption>Visualization of the simple Tensorflow model</figcaption>
	</figure>
      </a>
    </div>
    
    <h4>Model training</h4>
    Model training on CloudML is
    straightforward. Like Dataflow, CloudML has a set of APIs for
    commandline and Python to manage training job. The user can submit
    the model training job with options for the configuration training
    cluster. 
    
    <pre class="prettyprint">
      gcloud beta ml jobs submit training "$JOB_ID" \
      --module-name trainer.task \
      --package-path trainer \
      --staging-bucket "$BUCKET" \
      --region us-central1 \
      -- \
      --output_path "${GCS_PATH}/training" \
      --eval_data_paths "${GCS_PATH}/preproc/eval*" \
      --train_data_paths "${GCS_PATH}/preproc/train*"
      </pre>
    <figcaption>Submit a model training job of the flower model by
    commandline</figcaption>
    
    
    <h4>Model serving</h4>
    Model serving is another managed service included in CloudML. The
    users first create a model through commandline or Python API. Then
    a model namespace is created. Under this model namespace users can
    create different instances of inference model. The second step is
    to create a version of the model for each version it is a lived
    inference model running on a container/containers. CloudML
    automatically add API service in this inference model so users can
    acquire prediction results through provided REST API.

    <pre class="prettyprint">
      gcloud beta ml versions create "$VERSION_NAME" \
      --model "$MODEL_NAME" \
      --origin "${GCS_PATH}/training/model"
      gcloud beta ml versions set-default "$VERSION_NAME" --model "$MODEL_NAME"
      </pre>
    <figcaption>Create a model namespace and lived model instance for 
prediction</figcaption>

    <h3>Storage</h3>
    Cloud storage is used to store data and model in the machine
    learning workflow. The CloudML model training set the
    Cloud Storage as default location for input and output data. After
    training the model write into the Cloud Storage and served as a
    prediction service by CloudML.
    	
    <h3>Datalab</h3>
    Python notebook especially <a href="">Jupyter</a> is a convenient
    tool for most data scientist. The notebook can integrate python
    code and comments in a interactive environment and help data
    scientist to find insights and debug learning algorithm
    quickly. Datalab is a highly customized interactive environment
    for Python based on <code>Jupyter</code>. Datalab integrates
    TensorFlow, Beam and GCP APIs so users can directly write and run
    code to preprocess data and train model. 
    

    <h2>TBD</h2>

    
    <h2>Conclusion</h2>

    This article only examines basic ideas and
    usages of GCP to build machine learning workflow.


<h3>Acknowledgements</h3>

 We want to thank our colleagues *** from OCI for very useful and professional comments 
that greatly
improved quality of this article.


    <!--#include virtual="footer.shtml" -->
    <script src="http://alexgorbatchev.com/pub/sh/current/scripts/shCore.js" 
type="text/javascript"></script>
    <script src="http://alexgorbatchev.com/pub/sh/current/scripts/shAutoloader.js" 
type="text/javascript"></script>

    <script src="http://alexgorbatchev.com/pub/sh/current/scripts/shBrushJava.js" 
type="text/javascript"></script>
    <script src="http://alexgorbatchev.com/pub/sh/current/scripts/shBrushXml.js" 
type="text/javascript"></script>
    <script src="http://alexgorbatchev.com/pub/sh/current/scripts/shBrushPhp.js" 
type="text/javascript"></script>
    <script src="http://alexgorbatchev.com/pub/sh/current/scripts/shBrushScala.js" 
type="text/javascript"></script>
    <script src="http://alexgorbatchev.com/pub/sh/current/scripts/shBrushJScript.js" 
type="text/javascript"></script>
    <script src="http://alexgorbatchev.com/pub/sh/current/scripts/shBrushPlain.js" 
type="text/javascript"></script>
    <script src="http://alexgorbatchev.com/pub/sh/current/scripts/shBrushCpp.js" 
type="text/javascript"></script>
    <script src="http://alexgorbatchev.com/pub/sh/current/scripts/shBrushCSharp.js" 
type="text/javascript"></script>
    <script 
src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></scrip
t>
    <script type="text/javascript">
        SyntaxHighlighter.all()
    </script>

    <h2>Futher Reading</h2>
    <ul>
    <li> <a 
href="https://software.intel.com/sites/landingpage/IntrinsicsGuide/#techs=AVX">The 
Dataflow Model: A Practical Approach to Balancing
	Correctness, Latency, and Cost in Massive-Scale,
	Unbounded, Out-of-Order Data Processing</a>, techniqual
	details of dataflow implementation.
    </li>
    <li><a href=""></a></li>
    </ul>

     
</body>

</html>



