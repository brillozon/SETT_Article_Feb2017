<!DOCTYPE HTML>
<html>

<head>
    <meta charset="utf-8">

    <meta name="description" content="Docker vs Vagrant" />
    <meta name="keywords" content="SETT, OCI, Docker, Vagrant" />
    <meta name="author" content="Yong Fu" />
    <title>SETT Feb 2017 - CloudML</title>

    <link rel="alternate" type="application/rss+xml" title="RSS" 
href="http://ociweb.com/sett/rss.xml" />
    <link href="styles/SETT.css" rel="stylesheet" type="text/css" />


    <!--Used for syntax highlighting.  -->
    <link href="http://alexgorbatchev.com/pub/sh/current/styles/shCore.css" 
rel="stylesheet" type="text/css" />
    <link href="http://alexgorbatchev.com/pub/sh/current/styles/shThemeDefault.css" 
rel="stylesheet" type="text/css" />
</head>

<body>
    <!--#include virtual="header.shtml" -->

    <h1>Leveraging Prediction API's and CloudML for practical applications</h1>

    <p class="author">
        by
        <br/> Data Analytics Group
        <br/>Object Computing, Inc. (OCI)
    </p>

    <h2>Introduction</h2>
    <p>
      Google Cloud Platform and CloudML
    </p>
    <p>
      Machine learning workflow
    </p>
    <p>
      Machine learning workflow on Google Cloud Platform
    </p>
    
    <h2>Goole Prediction APIS</h2>
	
	<p>
	Google prediction API offers machine learning as a service. It learns from a user given 
	training data and provides pattern-matching and machine learning capabilities. 
	The prediction API can predict a numeric value or a categorical value that describes a 
	new data example. Using these capabilities there is a possibility of building applications 
	from spam detection to recommendation engines without actually worrying about building a model.
	</p>
	
	<p>
	The following are the range of use-cases that can be built using the capabilitis of 
	google prediction API:
	</p>
	<ul>
		<li> Predict future trends from a given historical series of data.
		<li> Detect if a given email is a spam
		<li> Recommend a product/movie to the user based on the interests of a similar user.
		<li> Identify whether a given user will default based on the credit usage history of the user.
		<li> Activity detection from smartphone sensor data using labelled activity data-sets. 
	</ul>
	
	<p>
	On a given labelled training dataset. Prediction API performs the following specific tasks:
	</p>
	<ul>
		<li> Given a new item, predict a numeric value for that item, based on similar valued examples in its training data (regression).
		<li> Given a new item, choose a category that describes it best, given a set of similar categorized items in its training data (classification).
	</ul>
	
	<p>
	The bottomline of all the above discussed applications is to predict a world state parameter (target label value)
	for an unknown example based on the past labelled data examples. The prediction API will take care
	of building best suitable models using google's fast and reliable computing resources. Most prediction
	queries take less then 200ms.
	</p>
	
	<h3>How does the prediction API works?</h3>
	<p>
	The implementation of the Google's prediction API can be pretty much defined as a blackbox approach. 
	There is no control on the model-selection, model-tuning and things that happen in the background during training. 
	The model configuration is restricted to specifying the class of problem whether it 
	is "Classification" Vs "Regression" during the data preparation for training process. 
	On a very highlevel, the information flow in the prediction API looks as shown in the following:
	</p>
	
	<div id="container">
      <a href="#">
	<figure>
	  <img src="./figures/prediction_api.jpg" alt="inception" width="1000" height="400"/>
	  <figcaption>Information flow of Predcition API</figcaption>
	</figure>
      </a>
    </div>
	
	<p>
	Input features can contain any type of data, and the API does not impose any constraints on input data types or 
	any configuration process. The important step is data preparation, as per the specified format that prediction 
	API will accept for training. The data simply looks like a big table, where each record is an input data example 
	and the labels (target value) are specified in the very first column in the training set. The only difference between 
	training and the testing sets is the first column will not be present in the testing set. 
	The biggest disadvantage however with this type of implementation is you cannot predict two different world state parameters (labels) at the same time, which otherwise 
	can be done with your own model implementaion. 
	</p>
	
	<p>
	Prediction API offers a simple way to train machine learning models through a RESTful interface. The training phase 
	is initialized by calling "trainedmodels.insert" method. 
	The training phase is asynchronous, where you can poll the API using "prediction.trainedmodels.get" method 
	to check the status of the training. In the response, when "trainingStatus" property changed to "Done" from "Running",
	when the model training is completed. Only after this, you can start making predictions for new data examples.
	</p>
	
	<p>
	The API provides with "trainedmodels.analyze method" which specifies the "modelDescription" that contains the 
	confusionMatrix in the JSON format. It will not provide any additional statistics like precision, recall and F-score which
	you can manually calculate. 
	</p>
	
	<p>
	For the model built, you can make predictions for new example data-sets by calling "trainedmodels.predict" method, this returns
	the parameters "outputLabel" (numeric or String) and "outputMulti" which is a probability measure for each prediction class. The "outputMulti" 
	is so useful in making final prediction programmatically by inspecting each class's probability taking the context of the problem in to consideration. 
	</p>
	
	<h3>Who is the audience for Prediction API's?</h3>
	
	<p>
	Traditionally to work with a machine leaning problem, you normally start with preprocessing your data
	by performing some steps like dealing with missing data, input normalization, dataset splitting in to training and validation sets. 
	Then comes the step of model selection, based on the correlations in your training data, you identify a model that fits your scenario. 
	</p>
	
	<p>
	With the prediction API, you donâ€™t need to worry about these steps, since the API automatically trains and tests a lot of complex models, tuned 
	with different parameters, so that the best one will be chosen for the final evaluation. The model which API finally comes up with would have been 
	the one you end up after so many iterations of tuning which involves manual selection of parameters. Even, the model evaluation is handled by the API itself. 
	All you need to worry and work about is to provide a valid data source in the required format, to the API. 
	Google's blackbox implementaion approach for prediction API makes it clear that it wants to ease the implementation to non-coders also. In a way, the mathematical
	expertise required to build, analyze the machine learning models, is eliminated by the capabilities of the API. One 
	can invest more time in problem formulation, data-collection and making a flaw-less end to end implementation, with prediction API.
	</p>

	<h3>Prediction API: A real use-case for sentiment analysis</h3>
	
	<p>
	To illustrate the prediction API, a simple binary classification problem is shown to classify positive or negative 
	sentiment from the  Twitter sentiment analysis corpus dataset (http://thinknook.com/twitter-sentiment-analysis-training
	-corpus-dataset-2012-09-22/). 
	</p>
	
	<p>
	For working with a machine learning problem, a most important and time-consuming part is defining the problem appropriately 
	and preparing the dataset accordingly. Before all that, it should be analyzed and made sure that if the problem can be solved 
	using machine learning approach in the first place. After all, not every problem is solvable in using machine learning. 
	</p>
	
	<p>
	There are two main things you need to clarify and make sure before starting the problem: 
	</p>
	<ul>
		<li>what is the type of the problem regression/classiffication, and accordingly making sure what are you going to predict/classify
		<li>making necessary assumptions, while not affecting the scope of the problem
	</ul>
	
	<p>
	For our current problem, since we are going to identify a predefined label "Negative" or "Positive", this is clearly a classification problem. 
	The second point is important as you can have a redundant data which will inturn affects the model accuracy. Typically, 
	if the features are too specific to the current data-set, you may end up with a very large generalization error, resulting in overfitting. 
	This is the piece you need to handle yourself, to better assist the Prediction API predict accurate labels for your test inputs. 
	However, preprocessing step is not mandatory for using Prediction API, you need it to build a good model corresponding to your training set.
	</p>
	
	<p>
	The data-set chosen for the current problem contains about 1.5 million rows,
	and 4 columns. Usually, there may be columns not of interest to a specific problem, so filtering those by including only 
	relevant features will lead to a better model. Accordingly we prepare our data and make sure to include the label in the first column as mentioned in the API 
	implementation above. We now setup the project, which involves creating a cloud platform project (you can also build on top of an existing one), 
	enable billing and enable prediction API for the project.
	</p>
	Note: A  globally unique project id is choses for project name and a number is assigned when the cloud platform  project is created. Detailed description
	on this are provided at https://developers.google.com/ad-exchange/rtb/open-bidder/google-app-guide.

	<p>
	Then create a bucket with globally unique name and add the training set file as 'csv' file to the bucket. For training the model, "prediction.trainedmodels.insert" method is called, 
	passing a unique name for this predictive model, and the bucket location of the training data as shown below. A full list of the methods are provided at https://developers.google.com/apis-explorer/#p/prediction/v1.6/prediction.
	</p>
<div>
    <pre class="prettyprint">
	POST https://www.googleapis.com/prediction/v1.6/projects/oci-analytics/trainedmodels?key={YOUR_API_KEY}
	{
		"id": "sentiment-identifier_12500",
		"storageDataLocation": "oci-prediction_api-demo/twitter_data_12500.csv"
	}
    </pre>
    <figcaption>Request format for initializing the training
</div>

<p>

A successful response looks like:
<div>
    <pre class="prettyprint">
	Response 200
	- Show headers -
	{
		"kind": "prediction#training",
		"id": "sentiment-identifier_12500",
		"selfLink": "https://www.googleapis.com/prediction/v1.6/projects/oci-analytics/trainedmodels/sentiment
		-identifier_12500",
		"storageDataLocation": "oci-prediction_api-demo/twitter_data_12500.csv"
	}
    </pre>
    <figcaption>Response for the training request
</div>

<p>
To check the status of Training, use the "prediction.trainedmodels.get" method, by passing the ID of the predictive model as shown below.
<!-- GET https://www.googleapis.com/prediction/v1.6/projects/[PROJECT_ID]/trainedmodels/sentiment-identifier -->
</p>

<div>
    <pre class="prettyprint">
		GET https://www.googleapis.com/prediction/v1.6/projects/[PROJECT_ID]/trainedmodels/sentiment-identifier
    </pre>
    <figcaption>Querying about the status of training
</div>

<p>
After the training is complete, you can send queries to the service to be evaluated against the predictive model. To do so, call the "prediction.trainedmodels.predict" method, 
passing the name of the model and the query.
</p>


<div>
    <pre class="prettyprint">
		POST https://www.googleapis.com/prediction/v1.6/projects/oci-analytics/trainedmodels/sentiment-identifier_12500/predict?key={YOUR_API_KEY}
		{
			"input": {
				"csvInstance": [
					"I am worried about today's game..."
				]
			}
		}
    </pre>
    <figcaption>Sending a prediction query to the Prediction API
</div>


<div>
    <pre class="prettyprint">
		200
		- Show headers -
		{
			"kind": "prediction#output",
			"id": "sentiment-identifier_12500",
			"selfLink": "https://www.googleapis.com/prediction/v1.6/projects/oci-analytics/trainedmodels/sentiment-identifier_12500/predict",
			"outputLabel": "NEGATIVE",
			"outputMulti": [
			{
				"label": "NEGATIVE",
				"score": "0.696235"
			},
			{
				"label": "POSITIVE",
				"score": "0.303765"
			}
			]
		}
    </pre>
    <figcaption>Prediction response containing the prediction label and probability scores
</div>
	
	<h3>Summary of the Prediction API</h3>
	<p>
	As of now, the prediction API seems to be abstracted very much to the developers. The control is only
	on the data preparation and adding additional data-sets for updating the model, these forms the either 
	end-points of the machine learning pipeline. Optimistically, a key advantage with this approach is, it saves lot of
	time in building the models, and also the flexibility of adding additional data-sets even after the training is completed, 
	resulting in model update on the fly.
	</p>
    
    <h2>Conclusion</h2>

    This article only examines basic ideas and
    usages of GCP to build machine learning workflow.


<h3>Acknowledgements</h3>

 We want to thank our colleagues *** from OCI for very useful and professional comments 
that greatly
improved quality of this article.


    <!--#include virtual="footer.shtml" -->
    <script src="http://alexgorbatchev.com/pub/sh/current/scripts/shCore.js" 
type="text/javascript"></script>
    <script src="http://alexgorbatchev.com/pub/sh/current/scripts/shAutoloader.js" 
type="text/javascript"></script>

    <script src="http://alexgorbatchev.com/pub/sh/current/scripts/shBrushJava.js" 
type="text/javascript"></script>
    <script src="http://alexgorbatchev.com/pub/sh/current/scripts/shBrushXml.js" 
type="text/javascript"></script>
    <script src="http://alexgorbatchev.com/pub/sh/current/scripts/shBrushPhp.js" 
type="text/javascript"></script>
    <script src="http://alexgorbatchev.com/pub/sh/current/scripts/shBrushScala.js" 
type="text/javascript"></script>
    <script src="http://alexgorbatchev.com/pub/sh/current/scripts/shBrushJScript.js" 
type="text/javascript"></script>
    <script src="http://alexgorbatchev.com/pub/sh/current/scripts/shBrushPlain.js" 
type="text/javascript"></script>
    <script src="http://alexgorbatchev.com/pub/sh/current/scripts/shBrushCpp.js" 
type="text/javascript"></script>
    <script src="http://alexgorbatchev.com/pub/sh/current/scripts/shBrushCSharp.js" 
type="text/javascript"></script>
    <script 
src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></scrip
t>
    <script type="text/javascript">
        SyntaxHighlighter.all()
    </script>

    <h2>Futher Reading</h2>
    <ul>
    <li> <a 
href="https://software.intel.com/sites/landingpage/IntrinsicsGuide/#techs=AVX">The 
Dataflow Model: A Practical Approach to Balancing
	Correctness, Latency, and Cost in Massive-Scale,
	Unbounded, Out-of-Order Data Processing</a>, techniqual
	details of dataflow implementation.
    </li>
    <li><a href=""></a></li>
    </ul>

     
</body>

</html>



